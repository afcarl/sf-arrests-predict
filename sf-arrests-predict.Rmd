---
title: "Predicting And Mapping Police Arrests in San Francisco with LightGBM and R"
author: "Max Woolf (@minimaxir)"
date: "January 2nd, 2017"
output:
  html_notebook:
    highlight: tango
    mathjax: null
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

This R Notebook is the complement to my blog post [Playing with 80 Million Amazon Product Review Ratings Using Apache Spark](http://minimaxir.com/2017/01/amazon-spark/).

This notebook is licensed under the MIT License. If you use the code or data visualization designs contained within this notebook, it would be greatly appreciated if proper attribution is given back to this notebook and/or myself. Thanks! :)

# Setup

Setup the R packages.

install.packages("/Users/maxwoolf/Downloads/xgboost/R-package", repos = NULL, type="source")

```{r setup}
library(lightgbm)
library(Matrix)
library(caret)

source("Rstart.R")
```

```{r}
sessionInfo()
```

Import data, and only keep relevant columns. Filter on Arrests only.

The data must be randomized for `lightgbm` to give unbiased scores. Can do with dplyr's `sample_frac`.

```{r, include=FALSE}
file_path <- "~/Downloads/SFPD_Incidents_-_from_1_January_2003.csv"

df <- read_csv(file_path, col_types="_c_ccc_c_nn__") %>%
        filter(grepl("ARREST", Resolution))
```

```{r}
# seed for sample_frac()
set.seed(123)

df <- df %>% sample_frac()

df %>% head()
```

# Feature Engineering

Engineer features for `lightgbm`.

## Month, Hour, Year

Year is # years since the lowest year (in this case, 2003, as noted in the dataset title)

```{r}
df <- df %>%
        mutate(month = factor(substring(Date, 1, 2)),
                hour = factor(substring(Time, 1, 2)),
                year = as.numeric(substring(Date, 7, 10)))

df %>% select(month, hour, year) %>% head()
```

## Existing DayOfWeek to Factor

Change DayOfWeek to Factor. (not strictly necessary in this case, but reduces memory usage)

```{r}
df <- df %>%
        mutate(DayOfWeek = factor(DayOfWeek))

df %>% select(DayOfWeek) %>% head()
```

## Category Indices

Map the category to an index. Labels must be zero-indexed.

```{r}
df <- df %>%
        mutate(category_index = as.numeric(factor(Category)) - 1)

df %>% select(category_index, Category) %>% head()
```

# lightgbm Training

Use LightGBM's categorical data feature for optimial performance.

Use `caret` for train/test splitting since `createDataPartition` ensures balanced distribution of categories between train and test.

```{r}
# proportion of data to train on
split <- 0.9

set.seed(123)
trainIndex <- createDataPartition(df$category_index, p = split, list = FALSE, times = 1)

dtrain <- lgb.Dataset((df %>% select(X, Y, hour, month, year, DayOfWeek) %>% data.matrix())[trainIndex,],
                     colnames = c("X", "Y", "hour", "month", "year", "DayOfWeek"),
                     categorical_feature = c("hour", "month", "DayOfWeek"),
                     label = df$category_index[trainIndex], free_raw_data=T)

dtest <- lgb.Dataset.create.valid(dtrain,
                                  (df %>% select(X, Y, hour, month, year, DayOfWeek) %>% data.matrix())[-trainIndex,],
                                  label = df$category_index[-trainIndex])

params <- list(objective = "multiclass", metric = "multi_logloss")
valids <- list(test=dtest)

num_classes <- length(unique(df$category_index))
```

The size of the training set is **`r length(trainIndex) %>% format(big.mark=",")`** and the size of the test set is **`r (df %>% nrow() - length(trainIndex)) %>% format(big.mark=",")`**.

```{r}
# determine elapsed runtime 
system.time(

# training output not printed to notebook since spammy. (verbose = 0 + record = T)
bst <- lgb.train(params,
                dtrain,
                nrounds = 500,
                valids,
                num_threads = 4,
                num_class = num_classes,
                verbose = 0,
                record = T,
                early_stopping_rounds = 5
                )

)[3]

# multilogloss of final iteration on test set
paste("# Rounds:", bst$current_iter())
paste("Multilogloss of best model:", bst$record_evals$test$multi_logloss$eval %>% unlist() %>% tail(1))
```


`preds` is a 1D vector of probabilities for each vector, of nrows x nclasses. Reshape accordingly and iterate through for the predicted label (label with the largest probability) and the corresponding probability.

```{r}
test <- (df %>% select(X, Y, hour, month, year, DayOfWeek) %>% data.matrix())[-trainIndex,]

# predict() has a reshape parameter but it doesn't work oddly
preds_matrix <- matrix(predict(bst, test), byrow=T, nrow(test), num_classes)

preds_cor <- cor(preds_matrix)

preds_matrix[1:6,]

results <- t(apply(preds_matrix, 1, function (x) {
  max_index = which(x==max(x))
  return (c(max_index-1, x[max_index]))
}))
```

```{r}
df_results <- data.frame(results, label_act = df$category_index[-trainIndex]) %>%
                tbl_df() %>%
                transmute(label_pred = X1, prod_pred = X2, label_act)

df_results %>% arrange(desc(prod_pred)) %>% head(20)

#rm(preds_matrix)
#rm(test)
```


Confusion matrix:

```{r}
cm <- confusionMatrix(df_results$label_pred, df_results$label_act)

data.frame(cm$overall)
```

# Visualizations

## Confusion Matrix

Plot the confusion matrix. Fortunately, matrix is already in long format.

```{r}
df_cm <- tbl_df(data.frame(cm$table))

df_cm %>% head(100)
```

Map the labels to the indices.

```{r}
# create mapping df
df_labels <- df %>%
              select(category_index, Category) %>%
              group_by(category_index, Category) %>%
              summarize() %>%
              ungroup() %>%
              mutate(category_index = factor(category_index))

df_cm <- df_cm %>%
                left_join(df_labels, by = c("Prediction" = "category_index")) %>%
                left_join(df_labels, by = c("Reference" = "category_index")) %>%
                rename(label_pred = Category.x, label_act = Category.y)

df_cm %>% head(100)
```

Plot the confusion matrix. Since 39 labels, confusion matrix will be large to fit all labels. Will also need to log-scale.

```{r}
# create a data frame of "correct values" to annotate
df_correct = df_cm %>% filter(label_pred == label_act)

plot <- ggplot(df_cm, aes(x=label_act, y=label_pred, fill = Freq)) +
          geom_tile() +
          geom_point(data=df_correct, color="white", size=0.8) +
          fte_theme() +
          scale_x_discrete() +
          scale_y_discrete() +
          theme(legend.title = element_text(size=7, family="Open Sans Condensed Bold"), legend.position="top", legend.direction="horizontal", legend.key.width=unit(1.75, "cm"), legend.key.height=unit(0.25, "cm"), legend.margin=unit(0,"cm"), panel.margin=element_blank(), axis.text.x=element_text(angle=-90)) +
            scale_fill_viridis(name="# of Preds", labels=comma, breaks=10^(0:4), trans="log10") +
            labs(title = "Confusion Matrix between 63,144 Predicted SFPD Arrest Labels and Actual",
                 x = "Actual Label of Arrest",
                 y = "Predicted Label of Arrest")

max_save(plot, "confusionMatrix", "SF Open Data", h=6, w=6, tall=T)
```


## Correlations

Covert the `preds_cor` matrix into long (adapted from http://stackoverflow.com/a/26838774)

```{r}
df_corr <- tbl_df(data.frame(Var1=c(row(preds_cor))-1, Var2=c(col(preds_cor))-1, value = c(preds_cor))) %>%
            filter(Var1 <= Var2) %>%
            mutate(Var1 = factor(Var1), Var2=factor(Var2))

df_corr %>% head(100)
```

Plot similar chart to confusion matrix. Requires reordering correlations for cleaner chart: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization)

```{r}
df_corr <- df_corr %>%
                left_join(df_labels, by = c("Var1" = "category_index")) %>%
                left_join(df_labels, by = c("Var2" = "category_index")) %>%
                mutate(label1 = factor(Category.x), label2 = factor(Category.y))

dd <- as.dist((1-preds_cor)/2)
hc <- hclust(dd)
label_order <- hc$order

levels(df_corr$label1) <- levels(df_corr$label1)[label_order]
levels(df_corr$label2) <- levels(df_corr$label2)[label_order]

plot <- ggplot(df_corr, aes(x=label1, y=label2, fill=value)) +
          geom_tile() +
          fte_theme() +
          scale_x_discrete() +
          scale_y_discrete() +
          coord_fixed() +
          theme(legend.title = element_text(size=7, family="Open Sans Condensed Bold"), legend.position="top", legend.direction="horizontal", legend.key.width=unit(1.75, "cm"), legend.key.height=unit(0.25, "cm"), legend.margin=unit(0,"cm"), panel.margin=element_blank(), axis.text.x=element_text(angle=-90), axis.title.y=element_blank(), axis.title.x=element_blank()) +
            scale_fill_gradient2(high = "#2ecc71", low = "#e74c3c", mid = "white", 
   midpoint = 0, limit = c(-0.5,0.5), 
   name="Pearson\nCorrelation", breaks=pretty_breaks(8))  +
            labs(title = "Confusion Matrix between 63,144 Predicted SFPD Arrest Labels and Actual")

max_save(plot, "correlationMatrix", "SF Open Data", h=6, w=6, tall=T)
```


# Code Which Did Not Work Out (One-Hot Encoding)

The categorical approach using LightGBM is better. Here is the former code using OHE.

Categorical Features must be factors for one-hot encoding.


Convert the factor variables into dummy variables: `model.matrix()` can do this in R natively. (via [Stack Overflow](http://stackoverflow.com/a/5048727))


```{r}
# model.matrix() adds an Intercept column: the "-1" removes it.
# Matrix converts the dense matrix to sparse (reduces memory footprint to 25%).
train <- Matrix(model.matrix(~ X + Y + hour + month + year + DayOfWeek - 1, df))
num_classes <- length(unique(df$category_index))
num_rows <- nrow(train)

train[1:10,]
```

The objective is `multi_logloss` since there are many classes. The `multiclass` objective returns a probability for each class.

Demo: https://github.com/Microsoft/LightGBM/blob/master/R-package/tests/testthat/test_basic.R#L29

```{r, include=FALSE}
set.seed(123)

bst <- lightgbm(data = train, label = df$category_index, nrounds = 200, nthreads=8, objective = "multiclass", metric="multi_logloss", num_class=num_classes, early_stopping_rounds = 3, nfolds=5, verbose = 0)
```

```{r}
preds <- predict(bst, train[1:2,])
preds
length(preds)
```