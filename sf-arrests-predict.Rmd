---
title: "Predicting And Mapping Future Arrests in San Francisco with LightGBM and R"
author: "Max Woolf (@minimaxir)"
date: "January 2nd, 2017"
output:
  html_notebook:
    highlight: tango
    mathjax: null
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

This R Notebook is the complement to my blog post [Playing with 80 Million Amazon Product Review Ratings Using Apache Spark](http://minimaxir.com/2017/01/amazon-spark/).

This notebook is licensed under the MIT License. If you use the code or data visualization designs contained within this notebook, it would be greatly appreciated if proper attribution is given back to this notebook and/or myself. Thanks! :)

# Setup

Setup the R packages.

install.packages("/Users/maxwoolf/Downloads/xgboost/R-package", repos = NULL, type="source")

```{r setup}
library(lightgbm)
library(Matrix)
library(caret)

source("Rstart.R")
```

```{r}
sessionInfo()
```

Import data, and only keep relevant columns. Filter on Arrests only.

The data must be randomized for `lightgbm` to give unbiased scores. Can do with dplyr's `sample_frac`.

```{r, include=FALSE}
file_path <- "~/Downloads/SFPD_Incidents_-_from_1_January_2003.csv"

df <- read_csv(file_path, col_types="_c_ccc_c_nn__") %>%
        filter(grepl("ARREST", Resolution))
```

```{r}
# seed for sample_frac()
set.seed(123)

df <- df %>% sample_frac()

df %>% head()
```

# Feature Engineering

Engineer features for `lightgbm`.

## Month, Hour, Year

Year is # years since the lowest year (in this case, 2003, as noted in the dataset title)

```{r}
df <- df %>%
        mutate(month = factor(substring(Date, 1, 2)),
                hour = factor(substring(Time, 1, 2)),
                year = as.numeric(substring(Date, 7, 10)) - 2003)

df %>% select(month, hour, year) %>% head()
```

## Existing DayOfWeek to Factor

Change DayOfWeek to Factor. (not strictly necessary in this case, but reduces memory usage)

```{r}
df <- df %>%
        mutate(DayOfWeek = factor(DayOfWeek))

df %>% select(DayOfWeek) %>% head()
```

## Category Indices

Map the category to an index. Labels must be zero-indexed.

```{r}
df <- df %>%
        mutate(category_index = as.numeric(factor(Category)) - 1)

df %>% select(category_index, Category) %>% head()
```

# lightgbm Training

Use LightGBM's categorical data feature for optimial performance.

Use `caret` for train/test splitting since `createDataPartition` ensures balanced distribution of categories between train and test.

```{r}
# proportion of data to train on
split <- 0.7

set.seed(123)
trainIndex <- createDataPartition(df$category_index, p = split, list = FALSE, times = 1)

dtrain <- lgb.Dataset((df %>% select(X, Y, hour, month, year, DayOfWeek) %>% data.matrix())[trainIndex,],
                     colnames = list("X", "Y", "hour", "month", "year", "DayOfWeek"),
                     categorical_feature = list("hour", "month", "DayOfWeek"),
                     label = df$category_index[trainIndex], free_raw_data=T)

dtest <- lgb.Dataset.create.valid(dtrain,
                                  (df %>% select(X, Y, hour, month, year, DayOfWeek) %>% data.matrix())[-trainIndex,],
                                  label = df$category_index[-trainIndex])

params <- list(objective = "multiclass", metric = "multi_logloss")
valids <- list(test=dtest)

num_classes <- length(unique(df$category_index))
```

```{r}
# determine elapsed runtime 
system.time(

# training output not printed to notebook since spammy. (verbose = 0 + record = T)
bst <- lgb.train(params,
                dtrain,
                nrounds = 500,
                valids,
                num_threads = 4,
                num_class = num_classes,
                verbose = 0,
                record = T,
                early_stopping_rounds = 5
                )

)[3]

# multilogloss of final iteration on test set
paste("# Rounds:", bst$current_iter())
paste("Multilogloss of best model:", bst$record_evals$test$multi_logloss$eval %>% unlist() %>% tail(1))
```


`preds` is a 1D vector of probabilities for each vector, of nrows x nclasses. Reshape accordingly and iterate through for the predicted label (label with the largest probability) and the corresponding probability.

```{r}
test <- (df %>% select(X, Y, hour, month, year, DayOfWeek) %>% data.matrix())[-trainIndex,]

# predict() has a reshape parameter but it doesn't work oddly
preds_matrix <- matrix(predict(bst, test), byrow=T, nrow(test), num_classes)

preds_cor <- cor(preds_matrix)

preds_matrix[1:6,]

results <- t(apply(preds_matrix, 1, function (x) {
  max_index = which(x==max(x))
  return (c(max_index-1, x[max_index]))
}))
```

```{r}
df_results <- data.frame(results, label_act = df$category_index[-trainIndex]) %>%
                tbl_df() %>%
                transmute(label_pred = X1, prod_pred = X2, label_act)

df_results %>% arrange(desc(prod_pred)) %>% head(20)

#rm(preds_matrix)
#rm(test)
```


Confusion matrix:

```{r}
confusionMatrix(df_results$label_pred, df_results$label_act)
```

# Code Which Did Not Work Out (One-Hot Encoding)

The categorical approach using LightGBM is better. Here is the former code using OHE.

Categorical Features must be factors for one-hot encoding.


Convert the factor variables into dummy variables: `model.matrix()` can do this in R natively. (via [Stack Overflow](http://stackoverflow.com/a/5048727))


```{r}
# model.matrix() adds an Intercept column: the "-1" removes it.
# Matrix converts the dense matrix to sparse (reduces memory footprint to 25%).
train <- Matrix(model.matrix(~ X + Y + hour + month + year + DayOfWeek - 1, df))
num_classes <- length(unique(df$category_index))
num_rows <- nrow(train)

train[1:10,]
```

The objective is `multi_logloss` since there are many classes. The `multiclass` objective returns a probability for each class.

Demo: https://github.com/Microsoft/LightGBM/blob/master/R-package/tests/testthat/test_basic.R#L29

```{r, include=FALSE}
set.seed(123)

bst <- lightgbm(data = train, label = df$category_index, nrounds = 200, nthreads=8, objective = "multiclass", metric="multi_logloss", num_class=num_classes, early_stopping_rounds = 3, nfolds=5, verbose = 0)
```

```{r}
preds <- predict(bst, train[1:2,])
preds
length(preds)
```