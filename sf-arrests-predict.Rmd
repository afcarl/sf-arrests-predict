---
title: "Predicting And Mapping Police Arrest Types in San Francisco with LightGBM and R"
author: "Max Woolf (@minimaxir)"
date: "January 2nd, 2017"
output:
  html_notebook:
    highlight: tango
    mathjax: null
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

This R Notebook is the complement to my blog post [Playing with 80 Million Amazon Product Review Ratings Using Apache Spark](http://minimaxir.com/2017/01/amazon-spark/).

This notebook is licensed under the MIT License. If you use the code or data visualization designs contained within this notebook, it would be greatly appreciated if proper attribution is given back to this notebook and/or myself. Thanks! :)

# Setup

Setup the R packages.

```{r setup}
library(lightgbm)
library(Matrix)
library(caret)
library(viridis)
library(ggmap)

source("Rstart.R")
```

```{r}
sessionInfo()
```

Import data, and only keep relevant columns. Filter on Arrests only.

The data must be randomized for `lightgbm` to give unbiased scores. Can do with dplyr's `sample_frac`.

```{r, include=FALSE}
# Converts "LARCENY/THEFT" to "Larceny/Theft", etc.
proper_case <- function(x) {
    return (gsub("\\b([A-Z])([A-Z]+)", "\\U\\1\\L\\2" , x, perl = TRUE))
}

file_path <- "~/Downloads/SFPD_Incidents_-_from_1_January_2003.csv"

df <- read_csv(file_path, col_types="_c_ccc_c_nn__") %>%
        filter(grepl("ARREST", Resolution)) %>%
        mutate(Category = proper_case(Category))
```

```{r}
# seed for sample_frac()
set.seed(123)

df <- df %>% sample_frac()

df %>% head()
```

The most recent arrest occured on **`r df$Date %>% max()`**.

# Feature Engineering

Engineer features for `lightgbm`.

## Month, Hour, Year

Year is # years since the lowest year (in this case, 2003, as noted in the dataset title)

```{r}
df <- df %>%
        mutate(month = factor(substring(Date, 1, 2)),
                hour = factor(substring(Time, 1, 2)),
                year = as.numeric(substring(Date, 7, 10)))

df %>% select(month, hour, year) %>% head()
```

## Existing DayOfWeek to Factor

Change DayOfWeek to Factor.

Since column become encoded as numeric instead of categorical, encode order of numerals such that Saturday and Sunday are adjacent for proper `lte`/`gte` behavior.

```{r}
dow_order <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")

df <- df %>%
        mutate(DayOfWeek = factor(DayOfWeek, levels=dow_order))

df %>% select(DayOfWeek) %>% head()
```

## Category Indices

Map the category to an index. Labels must be zero-indexed.

```{r}
df <- df %>%
        mutate(category_index = as.numeric(factor(Category)) - 1)

df %>% select(category_index, Category) %>% head()
```

# lightgbm Training

Use LightGBM's categorical data feature for optimial performance.

Use `caret` for train/test splitting since `createDataPartition` ensures balanced distribution of categories between train and test.

```{r}
# declare categorical feature names, if any
categoricals <- NULL

# proportion of data to train on
split <- 0.7

set.seed(123)
trainIndex <- createDataPartition(df$category_index, p = split, list = FALSE, times = 1)

dtrain <- lgb.Dataset((df %>% select(X, Y, hour, month, year, DayOfWeek) %>% data.matrix())[trainIndex,],
                     colnames = c("X", "Y", "hour", "month", "year", "DayOfWeek"),
                     categorical_feature = categoricals,
                     label = df$category_index[trainIndex], free_raw_data=T)

dtest <- lgb.Dataset.create.valid(dtrain,
                                  (df %>% select(X, Y, hour, month, year, DayOfWeek) %>% data.matrix())[-trainIndex,],
                                  label = df$category_index[-trainIndex])

params <- list(objective = "multiclass", metric = "multi_logloss")
valids <- list(test=dtest)

num_classes <- length(unique(df$category_index))

# preformat sizes for use in data visualizations later
train_size_format <- length(trainIndex) %>% format(big.mark=",")
test_size_format <- (df %>% nrow() - length(trainIndex)) %>% format(big.mark=",")
```

The size of the training set is **`r train_size_format`** and the size of the test set is **`r test_size_format`**.

```{r}
# determine elapsed runtime 
system.time(

# training output not printed to notebook since spammy. (verbose = 0 + record = T)
bst <- lgb.train(params,
                dtrain,
                nrounds = 500,
                valids,
                num_threads = 4,
                num_class = num_classes,
                verbose = 0,
                record = T,
                early_stopping_rounds = 5,
                categorical_feature = categoricals
                )

)[3]

# multilogloss of final iteration on test set
paste("# Rounds:", bst$current_iter())
paste("Multilogloss of best model:", bst$record_evals$test$multi_logloss$eval %>% unlist() %>% tail(1))
```
Calculate variable importance. (note: takes awhile since single-threaded)

```{r}
df_imp <- tbl_df(lgb.importance(bst, percentage = TRUE))
df_imp
```


`preds` is a 1D vector of probabilities for each vector, of nrows x nclasses. Reshape accordingly and iterate through for the predicted label (label with the largest probability) and the corresponding probability.

```{r}
test <- (df %>% select(X, Y, hour, month, year, DayOfWeek) %>% data.matrix())[-trainIndex,]

preds_matrix <- predict(bst, test, reshape=T)

preds_cor <- cor(preds_matrix)

preds_matrix[1:2,]

results <- t(apply(preds_matrix, 1, function (x) {
  max_index = which(x==max(x))
  return (c(max_index-1, x[max_index]))
}))
```

```{r}
df_results <- data.frame(results, label_act = df$category_index[-trainIndex]) %>%
                tbl_df() %>%
                transmute(label_pred = X1, prod_pred = X2, label_act)

df_results %>% arrange(desc(prod_pred)) %>% head(20)

rm(preds_matrix)
```


Confusion matrix:

```{r}
cm <- confusionMatrix(df_results$label_pred, df_results$label_act)

data.frame(cm$overall)
```

# Visualizations

## Importance Bar Chart

```{r}
df_imp$Feature <- factor(df_imp$Feature, levels=rev(df_imp$Feature))
```


```{r}
plot <- ggplot(df_imp, aes(x=Feature, y=Gain)) +
          geom_bar(stat="identity", fill="#34495e", alpha=0.9) +
          geom_text(aes(label=sprintf("%0.1f%%", Gain*100)), color="#34495e", hjust=-0.25, family="Open Sans Condensed Bold", size=2.5) +
          fte_theme() +
          coord_flip() +
          scale_y_continuous(limits = c(0, 0.4), labels=percent) +
   theme(plot.title=element_text(hjust=0.5), axis.title.y=element_blank()) +
          labs(title="Feature Importance for SF Arrest Type Model", y="% of Total Gain in LightGBM Model")

max_save(plot, "imp", "SF Open Data", h=2)
```

![](imp.png)
## Confusion Matrix

Plot the confusion matrix. Fortunately, matrix is already in long format.

```{r}
df_cm <- tbl_df(data.frame(cm$table))

df_cm %>% head(100)
```

Map the labels to the indices.

```{r}
# create mapping df
df_labels <- df %>%
              select(category_index, Category) %>%
              group_by(category_index, Category) %>%
              summarize() %>%
              ungroup() %>%
              mutate(category_index = factor(category_index))

df_cm <- df_cm %>%
                left_join(df_labels, by = c("Prediction" = "category_index")) %>%
                left_join(df_labels, by = c("Reference" = "category_index")) %>%
                rename(label_pred = Category.x, label_act = Category.y)

df_cm %>% head(100)
```

Plot the confusion matrix. Since 39 labels, confusion matrix will be large to fit all labels. Will also need to log-scale.

```{r}
# create a data frame of "correct values" to annotate
df_correct <- df_cm %>% filter(label_pred == label_act)

plot <- ggplot(df_cm, aes(x=label_act, y=label_pred, fill = Freq)) +
          geom_tile() +
          geom_point(data=df_correct, color="white", size=0.8) +
          fte_theme() +
          coord_equal() +
          scale_x_discrete() +
          scale_y_discrete() +
          theme(legend.title = element_text(size=7, family="Open Sans Condensed Bold"), legend.position="top", legend.direction="horizontal", legend.key.width=unit(1.25, "cm"), legend.key.height=unit(0.25, "cm"), legend.margin=unit(0,"cm"), axis.text.x=element_text(angle=-90, size=6, vjust=0.5, hjust=0), axis.text.y=element_text(size=6), plot.title = element_text(hjust=1)) +
            scale_fill_viridis(name="# of Preds", labels=comma, breaks=10^(0:4), trans="log10") +
            labs(title = sprintf("Confusion Matrix between %s Predicted SFPD Arrest Labels and Actual", test_size_format),
                 x = "Actual Label of Arrest",
                 y = "Predicted Label of Arrest")

max_save(plot, "confusionMatrix", "SF Open Data", h=6, w=5, tall=T)
```

![](confusionMatrix.png)

## Correlations

Covert the `preds_cor` matrix into long (adapted from http://stackoverflow.com/a/26838774)

Requires reordering correlations for cleaner chart: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization)

```{r}
dd <- as.dist((1-preds_cor)/2)
hc <- hclust(dd, "centroid")
label_order <- hc$order
preds_cor_reorder <- preds_cor[label_order, label_order]

df_corr <- tbl_df(data.frame(Var1=c(row(preds_cor_reorder))-1, Var2=c(col(preds_cor_reorder))-1, value = c(preds_cor_reorder))) %>%
            filter(Var1 <= Var2) %>%
            mutate(Var1 = factor(Var1), Var2=factor(Var2))

df_corr %>% head(100)
```

Plot similar chart to confusion matrix. 

```{r}
df_corr <- df_corr %>%
                left_join(df_labels, by = c("Var1" = "category_index")) %>%
                left_join(df_labels, by = c("Var2" = "category_index")) %>%
                mutate(label1 = factor(Category.x), label2 = factor(Category.y))

# fix the label order to the reordered order from the hclust
levels(df_corr$label1) <- levels(df_corr$label1)[label_order]
levels(df_corr$label2) <- levels(df_corr$label2)[label_order]

plot <- ggplot(df_corr, aes(x=label1, y=label2, fill=value)) +
          geom_tile() +
          fte_theme() +
          scale_x_discrete() +
          scale_y_discrete() +
          coord_fixed() +
          theme(legend.title = element_text(size=7, family="Open Sans Condensed Bold"), legend.position="top", legend.direction="horizontal", legend.key.width=unit(1.25, "cm"), legend.key.height=unit(0.25, "cm"), legend.margin=unit(0,"cm"), panel.margin=element_blank(), axis.text.x=element_text(angle=-90, vjust=0.5, hjust=0), axis.title.y=element_blank(), axis.title.x=element_blank(), plot.title=element_text(hjust=1, size=6)) +
            scale_fill_gradient2(high = "#2ecc71", low = "#e74c3c", mid = "white", 
   midpoint = 0, limit = c(-0.5,0.5), 
   name="Pearson\nCorrelation", breaks=pretty_breaks(8))  +
            labs(title = sprintf("Correlations between Predicted Multiclass Probabilities of %s SFPD Arrest Category Labels", test_size_format))

max_save(plot, "correlationMatrix", "SF Open Data", h=6, w=5, tall=T)
```

![](correlationMatrix.png)

# Mapping Arrests

Reusing my mapping previous SF code: https://github.com/minimaxir/sf-arrests-when-where/blob/master/crime_data_sf.ipynb

```{r}
bbox = c(-122.516441,37.702072,-122.37276,37.811818)
map <- get_map(location = bbox, source = "stamen", maptype = "toner-lite")
```

Create 40000 million latitude/longitude points in SF to simulate locations (200 points on x axis, 2000 points on y axis)

```{r}
grid_size = 200

df_points <- data.frame(expand.grid(X=seq(bbox[1], bbox[3], length.out=grid_size),
                                    Y=seq(bbox[2], bbox[4], length.out=grid_size)
)
)

df_points %>% head()
df_points %>% nrow()
```

Predict arrest types at each point on April 15th, 2017, at 8 PM.

Populate data with same format of data (i.e. add month, hour, year, DayOfWeek). Does not require much customization. (DayOfWeek is a Factor, however)

```{r}
date_target <- as.POSIXct("2017-04-15 20:00:00")

df_points <- df_points %>%
              mutate(hour = format(date_target, "%H"),
                    month = format(date_target, "%m"),
                    year = format(date_target, "%Y"),
                    DayOfWeek = which(levels(df$DayOfWeek) == format(date_target, "%A"))) %>%
            data.matrix()

df_points %>% head()
```

```{r}
preds_matrix <- matrix(predict(bst, df_points), byrow=T, nrow(df_points), num_classes)

results <- t(apply(preds_matrix, 1, function (x) {
  max_index = which(x==max(x))
  return (c(max_index-1, x[max_index]))
}))

rm(preds_matrix)
```

```{r}
df_results <- data.frame(X=df_points[,1], Y=df_points[,2], label=factor(results[,1]), prob=results[,2]) %>%
                tbl_df() %>%
                left_join(df_labels, by=c("label" = "category_index")) %>%
                mutate(Category = factor(Category))

df_results %>% head(20)
```

```{r}
plot <- ggmap(map) +
            geom_raster(data = df_results %>% filter(Category != "Other Offenses"), aes(x=X, y=Y, fill=Category), alpha=0.8, size=0) +
            coord_cartesian() +
            fte_theme() +
            scale_fill_brewer(palette = "Dark2") +
            theme(axis.text.x = element_blank(), axis.text.y = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank()) +
            theme(legend.title = element_text(size=7, family="Open Sans Condensed Bold"), legend.position="right", legend.key.width=unit(0.5, "cm"), legend.key.height=unit(2, "cm"), legend.margin=margin(1,0,1,0), plot.title=element_text(hjust=0, size=11)) +
            labs(title = sprintf("Locations of Predicted Types of Arrests in San Francisco on %s",
                 format(date_target, '%B %d, %Y at %I %p')))

max_save(plot, sprintf("crime-%s", format(date_target, '%Y-%m-%d-%H')), "SF Open Data", w = 6, h = 6, tall=T)
```

# Code Which Did Not Work Out (One-Hot Encoding)

The categorical approach using LightGBM is better. Here is the former code using OHE.

Categorical Features must be factors for one-hot encoding.


Convert the factor variables into dummy variables: `model.matrix()` can do this in R natively. (via [Stack Overflow](http://stackoverflow.com/a/5048727))


```{r}
# model.matrix() adds an Intercept column: the "-1" removes it.
# Matrix converts the dense matrix to sparse (reduces memory footprint to 25%).
train <- Matrix(model.matrix(~ X + Y + hour + month + year + DayOfWeek - 1, df))
num_classes <- length(unique(df$category_index))
num_rows <- nrow(train)

train[1:10,]
```

The objective is `multi_logloss` since there are many classes. The `multiclass` objective returns a probability for each class.

Demo: https://github.com/Microsoft/LightGBM/blob/master/R-package/tests/testthat/test_basic.R#L29

```{r, include=FALSE}
set.seed(123)

bst <- lightgbm(data = train, label = df$category_index, nrounds = 200, nthreads=8, objective = "multiclass", metric="multi_logloss", num_class=num_classes, early_stopping_rounds = 3, nfolds=5, verbose = 0)
```

```{r}
preds <- predict(bst, train[1:2,])
preds
length(preds)
```